{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.interpolate import griddata\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import dask.array as da\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from joblib import Parallel, delayed\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#Extra settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train df shape:  (86584, 14)\n",
      "Test df shape:  (6576, 13)\n"
     ]
    }
   ],
   "source": [
    "#Loading train and test data\n",
    "try:\n",
    "    train_df=pd.read_csv('/home/anuragverma/Desktop/Kaggle/GeoAI Ground-level NO2 _Zindi/Datasets/Train.csv')\n",
    "    test_df=pd.read_csv('/home/anuragverma/Desktop/Kaggle/GeoAI Ground-level NO2 _Zindi/Datasets/Test.csv')\n",
    "    print(\"Train df shape: \" ,train_df.shape)\n",
    "    print(\"Test df shape: \", test_df.shape)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print('File not loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Prep_linear(df1):\n",
    "    df=df1.copy()\n",
    "    numeric_columns =['Precipitation','LST','AAI','CloudFraction','TropopausePressure','GT_NO2']\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        # Fill missing values temporarily using linear interpolation\n",
    "        data_interpolated = df[col].interpolate(method='linear')\n",
    "        \n",
    "        # Handle cases where interpolation might still leave NaNs at the ends\n",
    "        if data_interpolated.isna().sum() > 0:\n",
    "            data_interpolated.fillna(method='bfill', inplace=True)\n",
    "            data_interpolated.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # Decompose the time series to extract the trend component\n",
    "        decomposition = seasonal_decompose(data_interpolated, model='additive', period=30)\n",
    "        trend = decomposition.trend\n",
    "        \n",
    "        # Handle cases where the trend might still have NaNs at the ends\n",
    "        trend.fillna(method='bfill', inplace=True)\n",
    "        trend.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # Replace original NaN values with the trend component\n",
    "        df[col] = df[col].combine_first(trend)\n",
    "        \n",
    "        # Fill any remaining NaN values with the mean of the column\n",
    "        df[col].fillna(value=df[col].mean(),inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def Prep_linear_test(df1):\n",
    "    df=df1.copy()\n",
    "    numeric_columns =['Precipitation','LST','AAI','CloudFraction','TropopausePressure']\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        # Fill missing values temporarily using linear interpolation\n",
    "        data_interpolated = df[col].interpolate(method='linear')\n",
    "        \n",
    "        # Handle cases where interpolation might still leave NaNs at the ends\n",
    "        if data_interpolated.isna().sum() > 0:\n",
    "            data_interpolated.fillna(method='bfill', inplace=True)\n",
    "            data_interpolated.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # Decompose the time series to extract the trend component\n",
    "        decomposition = seasonal_decompose(data_interpolated, model='additive', period=30)\n",
    "        trend = decomposition.trend\n",
    "        \n",
    "        # Handle cases where the trend might still have NaNs at the ends\n",
    "        trend.fillna(method='bfill', inplace=True)\n",
    "        trend.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # Replace original NaN values with the trend component\n",
    "        df[col] = df[col].combine_first(trend)\n",
    "        \n",
    "        # Fill any remaining NaN values with the mean of the column\n",
    "        df[col].fillna(value=df[col].mean(),inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "#Testingtg cubic interpolcation second\n",
    "\n",
    "\n",
    "\n",
    "def Prep_spline(df1):\n",
    "    df=df1.copy()\n",
    "    numeric_columns =['NO2_strat','NO2_total','NO2_trop']\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        # Fill missing values temporarily using linear interpolation\n",
    "        data_interpolated = df[col].interpolate(method='spline',order=2)\n",
    "        \n",
    "        # Handle cases where interpolation might still leave NaNs at the ends\n",
    "        if data_interpolated.isna().sum() > 0:\n",
    "            data_interpolated.fillna(method='bfill', inplace=True)\n",
    "            data_interpolated.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # Decompose the time series to extract the trend component\n",
    "        decomposition = seasonal_decompose(data_interpolated, model='additive', period=30)\n",
    "        trend = decomposition.trend\n",
    "        \n",
    "        # Handle cases where the trend might still have NaNs at the ends\n",
    "        trend.fillna(method='bfill', inplace=True)\n",
    "        trend.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # Replace original NaN values with the trend component\n",
    "        df[col] = df[col].combine_first(trend)\n",
    "        \n",
    "        # Fill any remaining NaN values with the mean of the column\n",
    "        df[col].fillna(value=df[col].mean(),inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LON              0.911077\n",
      "Precipitation    4.569618\n",
      "CloudFraction    1.236881\n",
      "NO2_total        4.169193\n",
      "NO2_trop         2.935214\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "['Precipitation', 'CloudFraction', 'NO2_total', 'NO2_trop']\n",
      "\n",
      "\n",
      "1.507939283863649\n"
     ]
    }
   ],
   "source": [
    "train_model3_df_prep_mix=Prep_linear(train_df)\n",
    "train_model3_df_prep_mix=Prep_spline(train_model3_df_prep_mix)\n",
    "\n",
    "test_model3_df_prep_mix=Prep_linear_test(test_df)\n",
    "test_model3_df_prep_mix=Prep_spline(test_model3_df_prep_mix)\n",
    "\n",
    "# Select only numeric columns for both train and test datasets\n",
    "train_model3_df_prep_mix = train_model3_df_prep_mix.select_dtypes(include=['number'])\n",
    "test_model3_df_prep_mix = test_model3_df_prep_mix.select_dtypes(include=['number'])\n",
    "\n",
    "# Separate the target variable 'GT_NO2' from the features in the training dataset\n",
    "train_model3_df_prep_mix_GT_NO2_mix = train_model3_df_prep_mix['GT_NO2']\n",
    "train_model3_df_prep_mix = train_model3_df_prep_mix.drop('GT_NO2', axis=1)\n",
    "\n",
    "#Checking skewness for all cols.\n",
    "Skewed_cols=train_model3_df_prep_mix.skew()[abs(train_model3_df_prep_mix.skew())>0.5].index.to_list()\n",
    "Skewed_cols.remove('LON')\n",
    "print(train_model3_df_prep_mix.skew()[abs(train_model3_df_prep_mix.skew())>0.5])\n",
    "print('\\n')\n",
    "print(Skewed_cols)\n",
    "print('\\n')\n",
    "\n",
    "#GT_NO2 is also skewed\n",
    "print(train_df['GT_NO2'].skew())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model3_df_prep_mix_roll_wind_mean_stddev_sorted=train_model3_df_prep_mix.copy()\n",
    "train_model3_df_prep_mix_roll_wind_mean_stddev_sorted['Date']=train_df['Date'].copy()\n",
    "\n",
    "#We will be sorting by date so creating a ordered PK to sort it back\n",
    "# Create an ordered PK\n",
    "train_model3_df_prep_mix_roll_wind_mean_stddev_sorted['PK'] = range(1, len(train_model3_df_prep_mix_roll_wind_mean_stddev_sorted) + 1)\n",
    "\n",
    "# Ensure 'Date' is in datetime format\n",
    "train_model3_df_prep_mix_roll_wind_mean_stddev_sorted['Date'] = pd.to_datetime(train_model3_df_prep_mix_roll_wind_mean_stddev_sorted['Date'])\n",
    "\n",
    "# Sort by 'LAT', 'LON', and 'Date'\n",
    "train_model3_df_prep_mix_roll_wind_mean_stddev_sorted = train_model3_df_prep_mix_roll_wind_mean_stddev_sorted.sort_values(by=['LAT', 'LON', 'Date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>Precipitation</th>\n",
       "      <th>LST</th>\n",
       "      <th>AAI</th>\n",
       "      <th>CloudFraction</th>\n",
       "      <th>NO2_strat</th>\n",
       "      <th>NO2_total</th>\n",
       "      <th>NO2_trop</th>\n",
       "      <th>TropopausePressure</th>\n",
       "      <th>Date</th>\n",
       "      <th>PK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>44.924694</td>\n",
       "      <td>10.517502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>280.675857</td>\n",
       "      <td>0.286079</td>\n",
       "      <td>0.954099</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>14436.75358</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2518</th>\n",
       "      <td>44.924694</td>\n",
       "      <td>10.517502</td>\n",
       "      <td>8.211939</td>\n",
       "      <td>279.766000</td>\n",
       "      <td>-0.579522</td>\n",
       "      <td>0.970421</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>16692.01730</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>2519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4730</th>\n",
       "      <td>44.924694</td>\n",
       "      <td>10.517502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>294.249333</td>\n",
       "      <td>-0.886214</td>\n",
       "      <td>0.273483</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>19279.33913</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>4731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7179</th>\n",
       "      <td>44.924694</td>\n",
       "      <td>10.517502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>294.240000</td>\n",
       "      <td>-0.894068</td>\n",
       "      <td>0.109390</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>19286.55321</td>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>7180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9549</th>\n",
       "      <td>44.924694</td>\n",
       "      <td>10.517502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>303.260000</td>\n",
       "      <td>-0.987795</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>19282.00083</td>\n",
       "      <td>2019-01-05</td>\n",
       "      <td>9550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            LAT        LON  Precipitation         LST       AAI  \\\n",
       "69    44.924694  10.517502       0.000000  280.675857  0.286079   \n",
       "2518  44.924694  10.517502       8.211939  279.766000 -0.579522   \n",
       "4730  44.924694  10.517502       0.000000  294.249333 -0.886214   \n",
       "7179  44.924694  10.517502       0.000000  294.240000 -0.894068   \n",
       "9549  44.924694  10.517502       0.000000  303.260000 -0.987795   \n",
       "\n",
       "      CloudFraction  NO2_strat  NO2_total  NO2_trop  TropopausePressure  \\\n",
       "69         0.954099   0.000024   0.000395  0.000140         14436.75358   \n",
       "2518       0.970421   0.000030   0.000873  0.000143         16692.01730   \n",
       "4730       0.273483   0.000037   0.000171  0.000119         19279.33913   \n",
       "7179       0.109390   0.000050   0.000152  0.000102         19286.55321   \n",
       "9549       0.000008   0.000063   0.000116  0.000053         19282.00083   \n",
       "\n",
       "           Date    PK  \n",
       "69   2019-01-01    70  \n",
       "2518 2019-01-02  2519  \n",
       "4730 2019-01-03  4731  \n",
       "7179 2019-01-04  7180  \n",
       "9549 2019-01-05  9550  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model3_df_prep_mix_roll_wind_mean_stddev_sorted.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model3_df_prep_mix_roll_wind_mean_stddev_sorted[train_model3_df_prep_mix_roll_wind_mean_stddev_sorted['LAT']==44.99954599].head(100)\n",
    "Rolling_window=3\n",
    "Cols_for_Rolling_Window=train_model3_df_prep_mix.columns.to_list()\n",
    "Cols_for_Rolling_Window.remove('LAT')\n",
    "Cols_for_Rolling_Window.remove('LON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>Precipitation</th>\n",
       "      <th>LST</th>\n",
       "      <th>AAI</th>\n",
       "      <th>CloudFraction</th>\n",
       "      <th>NO2_strat</th>\n",
       "      <th>NO2_total</th>\n",
       "      <th>NO2_trop</th>\n",
       "      <th>TropopausePressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45.601585</td>\n",
       "      <td>11.903551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>280.097333</td>\n",
       "      <td>0.230527</td>\n",
       "      <td>0.559117</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>14440.82126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45.371005</td>\n",
       "      <td>11.840830</td>\n",
       "      <td>3.047342</td>\n",
       "      <td>280.097333</td>\n",
       "      <td>-0.074006</td>\n",
       "      <td>0.869309</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>14441.79815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45.045825</td>\n",
       "      <td>12.060869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>280.097333</td>\n",
       "      <td>0.024470</td>\n",
       "      <td>0.674160</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>14437.38294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.104075</td>\n",
       "      <td>11.553241</td>\n",
       "      <td>1.200467</td>\n",
       "      <td>280.097333</td>\n",
       "      <td>-0.010442</td>\n",
       "      <td>0.920054</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>14440.83831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45.038758</td>\n",
       "      <td>11.790152</td>\n",
       "      <td>1.274564</td>\n",
       "      <td>280.097333</td>\n",
       "      <td>-0.176178</td>\n",
       "      <td>0.747464</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>14438.79037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         LAT        LON  Precipitation         LST       AAI  CloudFraction  \\\n",
       "0  45.601585  11.903551       0.000000  280.097333  0.230527       0.559117   \n",
       "1  45.371005  11.840830       3.047342  280.097333 -0.074006       0.869309   \n",
       "2  45.045825  12.060869       0.000000  280.097333  0.024470       0.674160   \n",
       "3  45.104075  11.553241       1.200467  280.097333 -0.010442       0.920054   \n",
       "4  45.038758  11.790152       1.274564  280.097333 -0.176178       0.747464   \n",
       "\n",
       "   NO2_strat  NO2_total  NO2_trop  TropopausePressure  \n",
       "0   0.000024   0.000117  0.000131         14440.82126  \n",
       "1   0.000024   0.000127  0.000131         14441.79815  \n",
       "2   0.000024   0.000086  0.000131         14437.38294  \n",
       "3   0.000024   0.000124  0.000131         14440.83831  \n",
       "4   0.000024   0.000116  0.000131         14438.79037  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model3_df_prep_mix.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add NT_GO2 to sorted before moving on"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
